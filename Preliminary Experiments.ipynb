{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b3337f-c03c-468a-94b0-e68660c4ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccesary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "# For Logistic Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# For LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# For BERT\n",
    "from datasets import Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2518034e-5fed-41d6-af33-5a5a3031e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Loading & Preprocessing\n",
    "# Load a small subset of dataset \n",
    "df = pd.read_csv('review.csv').sample(n=5000, random_state=42)\n",
    "\n",
    "# Remove rows with missing values and duplicates\n",
    "df.dropna(subset=['review', 'score'], inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Cleaning function for text\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "df['review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Map scores to sentiment labels (0: Negative, 1: Neutral, 2: Positive)\n",
    "def score_to_label(score):\n",
    "    if score <= 2:\n",
    "        return 0\n",
    "    elif score == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df['label'] = df['score'].apply(score_to_label)\n",
    "\n",
    "# For BERT, rename the review column to \"text\"\n",
    "bert_df = df[['review', 'label']].rename(columns={'review': 'text'})\n",
    "\n",
    "# Split the data for all\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(\"Data loaded and preprocessed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82908649-7268-41e0-9d30-36b1400a70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression Experiment ---\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.11      0.20        73\n",
      "           1       0.00      0.00      0.00        43\n",
      "           2       0.89      1.00      0.94       874\n",
      "\n",
      "    accuracy                           0.89       990\n",
      "   macro avg       0.59      0.37      0.38       990\n",
      "weighted avg       0.85      0.89      0.85       990\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zapor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\zapor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\zapor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Logistic Regression + TF-IDF\n",
    "print(\"\\n--- Logistic Regression Experiment ---\")\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['review'])\n",
    "X_test_tfidf = tfidf.transform(test_df['review'])\n",
    "\n",
    "# Train a simple Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "# Evaluate the model\n",
    "lr_preds = lr_model.predict(X_test_tfidf)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(test_df['label'], lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ad7eea-062b-48d1-aa96-388a82b4f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LSTM Experiment ---\n",
      "Epoch 1/3\n",
      "99/99 [==============================] - 14s 13ms/step - loss: 0.5211 - accuracy: 0.8699 - val_loss: 0.4369 - val_accuracy: 0.8851\n",
      "Epoch 2/3\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 0.4255 - accuracy: 0.8841 - val_loss: 0.4567 - val_accuracy: 0.8750\n",
      "Epoch 3/3\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 0.4055 - accuracy: 0.8923 - val_loss: 0.4266 - val_accuracy: 0.8876\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4267 - accuracy: 0.8828\n",
      "LSTM Test Accuracy: 0.8828282952308655\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: LSTM Model\n",
    "print(\"\\n--- LSTM Experiment ---\")\n",
    "# Tokenize the text \n",
    "vocab_size = 5000\n",
    "max_length = 100\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['review'])\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_df['review'])\n",
    "test_seq = tokenizer.texts_to_sequences(test_df['review'])\n",
    "train_pad = pad_sequences(train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Build a simple LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, input_length=max_length),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "lstm_model.fit(train_pad, train_df['label'].values, epochs=3, batch_size=32, validation_split=0.2)\n",
    "loss, acc = lstm_model.evaluate(test_pad, test_df['label'].values, verbose=1)\n",
    "print(\"LSTM Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00423514-9a84-4f1b-93f6-e59eb580a046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERT Experiment  ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0446773faa448e937212424fe820f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fabd6d6fed46f3b2edffdc68b2cb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\zapor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>0.258341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT (Fast Mode) Evaluation Results: {'eval_loss': 0.25834140181541443, 'eval_runtime': 6.3216, 'eval_samples_per_second': 15.819, 'eval_steps_per_second': 2.056, 'epoch': 1.0}\n",
      "BERT (Fast Mode) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71         8\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.95      0.99      0.97        87\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.54      0.58      0.56       100\n",
      "weighted avg       0.88      0.92      0.90       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zapor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\zapor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\zapor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: BERT \n",
    "print(\"\\n--- BERT Experiment  ---\")\n",
    "small_train_df = train_df.sample(n=1000, random_state=42)\n",
    "small_test_df = test_df.sample(n=100, random_state=42)\n",
    "\n",
    "# Create Hugging Face Datasets; rename review to text\n",
    "train_dataset = Dataset.from_pandas(small_train_df[['review', 'label']].rename(columns={'review': 'text'}))\n",
    "test_dataset = Dataset.from_pandas(small_test_df[['review', 'label']].rename(columns={'review': 'text'}))\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function \n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=64, truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Initialize model for sequence classification\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Define training arguments )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_fast\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Initialize Trainer \n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"BERT (Fast Mode) Evaluation Results:\", results)\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions_output.predictions, axis=1)\n",
    "print(\"BERT (Fast Mode) Classification Report:\")\n",
    "print(classification_report(predictions_output.label_ids, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35681a9b-afbd-4775-8000-f4e7c20cfdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
